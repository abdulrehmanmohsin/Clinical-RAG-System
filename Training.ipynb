{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fa5fa59e83443e39ee3f117369c7158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d956f81922e4db2a6511d6654a68892",
              "IPY_MODEL_4e033e1c9a1f439a925dcb7a53dd0d2d",
              "IPY_MODEL_ad2821b4414047f3915bda2dab53a6c9"
            ],
            "layout": "IPY_MODEL_fa8dfe15906442389c5c59464eb45101"
          }
        },
        "2d956f81922e4db2a6511d6654a68892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_064b03538d8d4c5483c7119dbb8808a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_25b081b920434767871c67ee75d65c72",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "4e033e1c9a1f439a925dcb7a53dd0d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c017fc45655e4a6b88f2c34ade59bb78",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2119108db5b45de96c2cd2281665cec",
            "value": 133466304
          }
        },
        "ad2821b4414047f3915bda2dab53a6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4120cabfcf6a40369b9d43d979a17cca",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bc352bb620344f08a668f91e2d52a2cb",
            "value": "â€‡133M/133Mâ€‡[00:14&lt;00:00,â€‡43.1MB/s]"
          }
        },
        "fa8dfe15906442389c5c59464eb45101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064b03538d8d4c5483c7119dbb8808a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b081b920434767871c67ee75d65c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c017fc45655e4a6b88f2c34ade59bb78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2119108db5b45de96c2cd2281665cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4120cabfcf6a40369b9d43d979a17cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc352bb620344f08a668f91e2d52a2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00876cfd527a4ada8749232b67af296f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a117a98f5e7486da617a0c9e4c63672",
              "IPY_MODEL_4efb3b78317c4bfea5459b2a922d6a62",
              "IPY_MODEL_31ef33c5aa0c41d980bde0a6e3adc1ac"
            ],
            "layout": "IPY_MODEL_7fad0ff974d24b6e84397b991db5238d"
          }
        },
        "5a117a98f5e7486da617a0c9e4c63672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2773493081db4e7995ae9cf4928c60b6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_429ab2b09d2d41abae272286b818f7e4",
            "value": "Fetchingâ€‡3â€‡files:â€‡â€‡â€‡0%"
          }
        },
        "4efb3b78317c4bfea5459b2a922d6a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0eaa9c24fa84aa0aec516f5d610bdeb",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a8eba9a833c4c47b2eb3246c068d603",
            "value": 0
          }
        },
        "31ef33c5aa0c41d980bde0a6e3adc1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db2be1303ca74f979509047d9fcae16d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8e5802e777064950bda6ef45de0e590b",
            "value": "â€‡0/3â€‡[00:00&lt;?,â€‡?it/s]"
          }
        },
        "7fad0ff974d24b6e84397b991db5238d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2773493081db4e7995ae9cf4928c60b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "429ab2b09d2d41abae272286b818f7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0eaa9c24fa84aa0aec516f5d610bdeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8eba9a833c4c47b2eb3246c068d603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db2be1303ca74f979509047d9fcae16d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e5802e777064950bda6ef45de0e590b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e22cb1f9e0849dab05bd6d60952add2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7ca8e508d9d4e64b69d61b08ae76808",
              "IPY_MODEL_75ae0bd0f7a24ee8952a3d22abdce582",
              "IPY_MODEL_9cb4ca3ca94041cda33451489c40f575"
            ],
            "layout": "IPY_MODEL_0ebf4fb42e1c415291c804b9b1763aba"
          }
        },
        "b7ca8e508d9d4e64b69d61b08ae76808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_918ba2e9c1134e67bf57619d0696cf3b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_703103da3dcc4eed950ef8a3927ceda4",
            "value": "model-00001-of-00003.safetensors:â€‡â€‡59%"
          }
        },
        "75ae0bd0f7a24ee8952a3d22abdce582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb97fbdeb1e9442b851c4e21d28d843f",
            "max": 4943162336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186fba0696184dc29ae0b069cb44a5ed",
            "value": 2928371059
          }
        },
        "9cb4ca3ca94041cda33451489c40f575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a132c733524184a8aeea8567471b77",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_95ee2c54d2bc4448aaab7e14f9db74a4",
            "value": "â€‡2.93G/4.94Gâ€‡[03:19&lt;02:10,â€‡15.4MB/s]"
          }
        },
        "0ebf4fb42e1c415291c804b9b1763aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "918ba2e9c1134e67bf57619d0696cf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "703103da3dcc4eed950ef8a3927ceda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb97fbdeb1e9442b851c4e21d28d843f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186fba0696184dc29ae0b069cb44a5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7a132c733524184a8aeea8567471b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95ee2c54d2bc4448aaab7e14f9db74a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes sentence-transformers\n",
        "!pip install -q chromadb pandas pyarrow tqdm streamlit\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "print(\"âœ“ All packages installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xs57j2jCckI",
        "outputId": "1f7361c4-8a46-4187-e21f-6a8b82157c9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All packages installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CELL 2: IMPORTS AND MOUNT GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "9hQ60JPbDDll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import time\n",
        "import re\n",
        "import hashlib\n",
        "import uuid\n",
        "import gc\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "from typing import List, Dict, Optional, Tuple, Callable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Transformers and quantization\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "# Embeddings and vector DB\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Streamlit\n",
        "import streamlit as st\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"âœ“ Google Drive mounted\")\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âœ“ Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"  GPU: {gpu_name}\")\n",
        "    print(f\"  Memory: {gpu_memory:.2f}GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLxj2HzRC_3V",
        "outputId": "89874e2f-715d-45d6-ff81-6e9b0542deda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Google Drive mounted\n",
            "âœ“ Device: cuda\n",
            "  GPU: Tesla T4\n",
            "  Memory: 15.83GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 3: CONFIGURATION CLASS"
      ],
      "metadata": {
        "id": "XowFfDFeDKFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Central configuration for the RAG pipeline with Google Drive paths.\"\"\"\n",
        "\n",
        "    # Google Drive Paths\n",
        "    DRIVE_ROOT = Path(\"/content/drive/MyDrive/Clinical_RAG_System\")\n",
        "    DATASET_ROOT = Path(\"/content/drive/MyDrive/mimic-iv-ext-direct-1.0.0/mimic-iv-ext-direct-1.0.0/Finished\")\n",
        "\n",
        "    # Persistent storage directories\n",
        "    PROCESSED_DATA_DIR = DRIVE_ROOT / \"processed_data\"\n",
        "    CHROMA_DB_PATH = DRIVE_ROOT / \"chroma_db\"\n",
        "    MODELS_CACHE = DRIVE_ROOT / \"models_cache\"\n",
        "    CHECKPOINTS_DIR = DRIVE_ROOT / \"checkpoints\"\n",
        "\n",
        "    # Cache files\n",
        "    PROCESSED_DF_PATH = PROCESSED_DATA_DIR / \"processed_documents.parquet\"\n",
        "    EMBEDDINGS_PATH = PROCESSED_DATA_DIR / \"embeddings.npy\"\n",
        "    PROCESSING_STATS_PATH = PROCESSED_DATA_DIR / \"processing_stats.json\"\n",
        "\n",
        "    # Models\n",
        "    EMBEDDING_MODEL = \"intfloat/e5-small-v2\"\n",
        "    GENERATION_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "    # Processing\n",
        "    CHUNK_SIZE_TOKENS = 400\n",
        "    CHUNK_OVERLAP_TOKENS = 100\n",
        "    MIN_TEXT_LENGTH = 50\n",
        "    BATCH_SIZE = 1000\n",
        "\n",
        "    # Generation\n",
        "    MAX_NEW_TOKENS = 512\n",
        "    TEMPERATURE = 0.7\n",
        "    TOP_P = 0.9\n",
        "    TOP_K = 50\n",
        "    REPETITION_PENALTY = 1.1\n",
        "    MAX_CONTEXT_TOKENS = 2000\n",
        "    MAX_INPUT_LENGTH = 4096\n",
        "\n",
        "    # Retrieval\n",
        "    DEFAULT_TOP_K = 5\n",
        "    QUERY_PREFIX = \"query: \"\n",
        "    PASSAGE_PREFIX = \"passage: \"\n",
        "\n",
        "    @classmethod\n",
        "    def setup_directories(cls):\n",
        "        \"\"\"Create necessary directories in Google Drive.\"\"\"\n",
        "        for path in [cls.DRIVE_ROOT, cls.PROCESSED_DATA_DIR,\n",
        "                     cls.CHROMA_DB_PATH, cls.MODELS_CACHE, cls.CHECKPOINTS_DIR]:\n",
        "            path.mkdir(parents=True, exist_ok=True)\n",
        "        print(\"âœ“ Directories created/verified\")\n",
        "\n",
        "    @classmethod\n",
        "    def check_cached_data(cls):\n",
        "        \"\"\"Check what data is already cached.\"\"\"\n",
        "        status = {\n",
        "            'processed_df': cls.PROCESSED_DF_PATH.exists(),\n",
        "            'embeddings': cls.EMBEDDINGS_PATH.exists(),\n",
        "            'chromadb': (cls.CHROMA_DB_PATH / \"chroma.sqlite3\").exists(),\n",
        "            'stats': cls.PROCESSING_STATS_PATH.exists()\n",
        "        }\n",
        "        return status\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "config.setup_directories()\n",
        "\n",
        "# Check cache status\n",
        "cache_status = config.check_cached_data()\n",
        "print(\"\\nðŸ“¦ Cache Status:\")\n",
        "for item, exists in cache_status.items():\n",
        "    status = \"âœ“ Exists\" if exists else \"âœ— Missing\"\n",
        "    print(f\"  {item}: {status}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPhix_1WDLfN",
        "outputId": "8934b5d4-36fd-4f0e-a2c4-62733197984d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Directories created/verified\n",
            "\n",
            "ðŸ“¦ Cache Status:\n",
            "  processed_df: âœ“ Exists\n",
            "  embeddings: âœ“ Exists\n",
            "  chromadb: âœ“ Exists\n",
            "  stats: âœ“ Exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 4: UTILITY FUNCTIONS"
      ],
      "metadata": {
        "id": "qwur5BFsDOAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_memory():\n",
        "    \"\"\"Aggressive GPU memory cleanup.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def get_gpu_memory():\n",
        "    \"\"\"Get current GPU memory statistics.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return {\n",
        "            'allocated': torch.cuda.memory_allocated(0) / 1e9,\n",
        "            'reserved': torch.cuda.memory_reserved(0) / 1e9,\n",
        "            'total': torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
        "            'free': (torch.cuda.get_device_properties(0).total_memory -\n",
        "                    torch.cuda.memory_reserved(0)) / 1e9\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean clinical text while preserving medical terminology.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'\\[\\*\\*[^\\]]+\\*\\*\\]', '[REDACTED]', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
        "    text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\t')\n",
        "\n",
        "    return text\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"Simple whitespace tokenization.\"\"\"\n",
        "    return text.split()\n",
        "\n",
        "def chunk_text_by_tokens(text: str, chunk_size: int = 400, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks based on token count.\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = len(simple_tokenize(sentence))\n",
        "\n",
        "        if current_tokens + sentence_tokens > chunk_size and current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "            overlap_text = ' '.join(current_chunk)\n",
        "            overlap_tokens = simple_tokenize(overlap_text)\n",
        "\n",
        "            if len(overlap_tokens) > overlap:\n",
        "                current_chunk = [' '.join(overlap_tokens[-overlap:])]\n",
        "                current_tokens = overlap\n",
        "            else:\n",
        "                current_chunk = []\n",
        "                current_tokens = 0\n",
        "\n",
        "        current_chunk.append(sentence)\n",
        "        current_tokens += sentence_tokens\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks if chunks else [text]\n",
        "\n",
        "def generate_doc_id(file_path: Path) -> str:\n",
        "    \"\"\"Generate unique document ID from file path hash.\"\"\"\n",
        "    hash_obj = hashlib.md5(str(file_path).encode('utf-8'))\n",
        "    return str(uuid.UUID(hash_obj.hexdigest()))\n",
        "\n",
        "def extract_text_from_json(data: dict) -> str:\n",
        "    \"\"\"Extract clinical text from JSON data structure.\"\"\"\n",
        "    text_fields = ['text', 'note', 'content', 'clinical_note', 'report',\n",
        "                   'description', 'narrative', 'summary', 'findings']\n",
        "\n",
        "    for field in text_fields:\n",
        "        if field in data and isinstance(data[field], str):\n",
        "            return data[field]\n",
        "\n",
        "    text_parts = [str(value) for key, value in data.items()\n",
        "                  if isinstance(value, str) and len(value) > 20]\n",
        "\n",
        "    return ' '.join(text_parts) if text_parts else \"\"\n",
        "\n",
        "def save_checkpoint(data: dict, checkpoint_name: str):\n",
        "    \"\"\"Save checkpoint to Google Drive.\"\"\"\n",
        "    checkpoint_path = config.CHECKPOINTS_DIR / f\"{checkpoint_name}.pkl\"\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "    print(f\"âœ“ Checkpoint saved: {checkpoint_name}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_name: str) -> Optional[dict]:\n",
        "    \"\"\"Load checkpoint from Google Drive.\"\"\"\n",
        "    checkpoint_path = config.CHECKPOINTS_DIR / f\"{checkpoint_name}.pkl\"\n",
        "    if checkpoint_path.exists():\n",
        "        with open(checkpoint_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "print(\"âœ“ Utility functions loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw2bQUzIDPmN",
        "outputId": "d710d104-3495-402b-8882-869a319b080c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Utility functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 5: DATA PROCESSOR WITH PROGRESS TRACKING"
      ],
      "metadata": {
        "id": "MJmqKq5TDSfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Process MIMIC-IV-EXT dataset into chunks for RAG with progress tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.stats = {\n",
        "            'files_processed': 0,\n",
        "            'files_skipped': 0,\n",
        "            'total_chunks': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "    def collect_json_files(self) -> List[Dict]:\n",
        "        \"\"\"Collect all JSON files from dataset.\"\"\"\n",
        "        print(\"ðŸ” Scanning dataset directory...\")\n",
        "        all_files = []\n",
        "\n",
        "        for root, dirs, files in os.walk(self.config.DATASET_ROOT):\n",
        "            root_path = Path(root)\n",
        "            for file in files:\n",
        "                if file.endswith('.json') and not file.startswith('.'):\n",
        "                    file_path = root_path / file\n",
        "                    relative_path = file_path.relative_to(self.config.DATASET_ROOT)\n",
        "                    path_parts = relative_path.parts\n",
        "\n",
        "                    all_files.append({\n",
        "                        'path': file_path,\n",
        "                        'disease_category': path_parts[0] if len(path_parts) > 0 else \"Unknown\",\n",
        "                        'disease_subtype': path_parts[1] if len(path_parts) > 1 else \"root\",\n",
        "                        'filename': file\n",
        "                    })\n",
        "\n",
        "        print(f\"âœ“ Found {len(all_files)} JSON files\")\n",
        "        return all_files\n",
        "\n",
        "    def process_files(self, json_files: List[Dict]) -> pd.DataFrame:\n",
        "        \"\"\"Process JSON files and create chunks with progress bar.\"\"\"\n",
        "        processed_documents = []\n",
        "\n",
        "        print(f\"\\nðŸ“ Processing {len(json_files)} files...\")\n",
        "\n",
        "        # Create progress bar\n",
        "        pbar = tqdm(json_files, desc=\"Processing files\", unit=\"file\")\n",
        "\n",
        "        for file_info in pbar:\n",
        "            try:\n",
        "                with open(file_info['path'], 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                if not isinstance(data, dict):\n",
        "                    self.stats['files_skipped'] += 1\n",
        "                    continue\n",
        "\n",
        "                raw_text = extract_text_from_json(data)\n",
        "\n",
        "                if not raw_text or len(raw_text) < self.config.MIN_TEXT_LENGTH:\n",
        "                    self.stats['files_skipped'] += 1\n",
        "                    continue\n",
        "\n",
        "                cleaned_text = clean_text(raw_text)\n",
        "\n",
        "                if not cleaned_text or len(cleaned_text) < self.config.MIN_TEXT_LENGTH:\n",
        "                    self.stats['files_skipped'] += 1\n",
        "                    continue\n",
        "\n",
        "                doc_id = generate_doc_id(file_info['path'])\n",
        "\n",
        "                chunks = chunk_text_by_tokens(\n",
        "                    cleaned_text,\n",
        "                    chunk_size=self.config.CHUNK_SIZE_TOKENS,\n",
        "                    overlap=self.config.CHUNK_OVERLAP_TOKENS\n",
        "                )\n",
        "\n",
        "                for chunk_idx, chunk_text in enumerate(chunks):\n",
        "                    if len(chunk_text) < self.config.MIN_TEXT_LENGTH:\n",
        "                        continue\n",
        "\n",
        "                    chunk_id = f\"{doc_id}_chunk_{chunk_idx}\"\n",
        "\n",
        "                    metadata = {k: v for k, v in data.items()\n",
        "                               if k not in ['text', 'note', 'content', 'clinical_note',\n",
        "                                          'report', 'description', 'narrative', 'summary']}\n",
        "\n",
        "                    doc_record = {\n",
        "                        'doc_id': doc_id,\n",
        "                        'chunk_id': chunk_id,\n",
        "                        'text': chunk_text,\n",
        "                        'disease_category': file_info['disease_category'],\n",
        "                        'disease_subtype': file_info['disease_subtype'],\n",
        "                        'source_file': str(file_info['path']),\n",
        "                        'chunk_index': chunk_idx,\n",
        "                        'total_chunks': len(chunks),\n",
        "                        'metadata': json.dumps(metadata, ensure_ascii=False)\n",
        "                    }\n",
        "\n",
        "                    processed_documents.append(doc_record)\n",
        "\n",
        "                self.stats['files_processed'] += 1\n",
        "                self.stats['total_chunks'] += len(chunks)\n",
        "\n",
        "                # Update progress bar description\n",
        "                pbar.set_postfix({\n",
        "                    'processed': self.stats['files_processed'],\n",
        "                    'chunks': self.stats['total_chunks']\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                self.stats['files_skipped'] += 1\n",
        "                self.stats['errors'].append({'file': str(file_info['path']), 'error': str(e)})\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        print(f\"\\nâœ“ Processing complete:\")\n",
        "        print(f\"  Files processed: {self.stats['files_processed']}\")\n",
        "        print(f\"  Files skipped: {self.stats['files_skipped']}\")\n",
        "        print(f\"  Total chunks created: {self.stats['total_chunks']}\")\n",
        "        print(f\"  Errors: {len(self.stats['errors'])}\")\n",
        "\n",
        "        df = pd.DataFrame(processed_documents)\n",
        "        df = df.drop_duplicates(subset=['text'], keep='first')\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "        print(f\"  Unique chunks after deduplication: {len(df)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save_processed_data(self, df: pd.DataFrame) -> Path:\n",
        "        \"\"\"Save processed DataFrame to parquet in Google Drive.\"\"\"\n",
        "        print(\"\\nðŸ’¾ Saving processed data to Google Drive...\")\n",
        "        output_path = self.config.PROCESSED_DF_PATH\n",
        "        df.to_parquet(output_path, index=False, compression='snappy')\n",
        "\n",
        "        # Save statistics\n",
        "        with open(self.config.PROCESSING_STATS_PATH, 'w') as f:\n",
        "            json.dump(self.stats, f, indent=2)\n",
        "\n",
        "        print(f\"âœ“ Data saved: {output_path}\")\n",
        "        print(f\"  Size: {output_path.stat().st_size / 1e6:.2f} MB\")\n",
        "        return output_path\n",
        "\n",
        "    def load_processed_data(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Load processed data from Google Drive if exists.\"\"\"\n",
        "        if self.config.PROCESSED_DF_PATH.exists():\n",
        "            print(\"ðŸ“‚ Loading cached processed data from Google Drive...\")\n",
        "            df = pd.read_parquet(self.config.PROCESSED_DF_PATH)\n",
        "\n",
        "            if self.config.PROCESSING_STATS_PATH.exists():\n",
        "                with open(self.config.PROCESSING_STATS_PATH, 'r') as f:\n",
        "                    self.stats = json.load(f)\n",
        "\n",
        "            print(f\"âœ“ Loaded {len(df)} chunks from cache\")\n",
        "            print(f\"  Files processed: {self.stats.get('files_processed', 'N/A')}\")\n",
        "            print(f\"  Total chunks: {self.stats.get('total_chunks', 'N/A')}\")\n",
        "            return df\n",
        "        return None\n",
        "\n",
        "print(\"âœ“ DataProcessor class loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSp85-6bDVj9",
        "outputId": "7299de8a-07f6-4d26-b74e-654dce232661"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ DataProcessor class loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 6: RUN DATA PROCESSING (Skip if cached)"
      ],
      "metadata": {
        "id": "KFzlmXwKDiUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 1: DATA PROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "processor = DataProcessor(config)\n",
        "\n",
        "# Try to load from cache first\n",
        "df = processor.load_processed_data()\n",
        "\n",
        "if df is None:\n",
        "    print(\"\\nâš  No cached data found. Processing dataset...\")\n",
        "    print(\"â± This will take 10-30 minutes depending on dataset size\\n\")\n",
        "\n",
        "    # Collect files\n",
        "    json_files = processor.collect_json_files()\n",
        "\n",
        "    # Process files\n",
        "    df = processor.process_files(json_files)\n",
        "\n",
        "    # Save to Google Drive\n",
        "    processor.save_processed_data(df)\n",
        "\n",
        "    print(\"\\nâœ“ Processing complete and saved to Google Drive!\")\n",
        "else:\n",
        "    print(\"\\nâœ“ Using cached processed data from Google Drive!\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nðŸ“Š Sample of processed data:\")\n",
        "display(df.head())\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Dataset Statistics:\")\n",
        "print(f\"  Total chunks: {len(df)}\")\n",
        "print(f\"  Unique documents: {df['doc_id'].nunique()}\")\n",
        "print(f\"  Disease categories: {df['disease_category'].nunique()}\")\n",
        "print(f\"  Average chunk length: {df['text'].str.len().mean():.0f} characters\")\n",
        "\n",
        "# Save reference for later cells\n",
        "globals()['processed_df'] = df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "4aLoZW98DjzE",
        "outputId": "4ec1cb20-9ee4-453b-fd48-e4d2e7a5e56b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: DATA PROCESSING\n",
            "================================================================================\n",
            "ðŸ“‚ Loading cached processed data from Google Drive...\n",
            "âœ“ Loaded 934 chunks from cache\n",
            "  Files processed: 511\n",
            "  Total chunks: 937\n",
            "\n",
            "âœ“ Using cached processed data from Google Drive!\n",
            "\n",
            "ðŸ“Š Sample of processed data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                 doc_id  \\\n",
              "0  6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e   \n",
              "1  6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e   \n",
              "2  4e290b0a-219e-09a3-c65f-f3c995c9f987   \n",
              "3  4e290b0a-219e-09a3-c65f-f3c995c9f987   \n",
              "4  2b0f1a6c-5f6a-fabc-7f0f-014323655c78   \n",
              "\n",
              "                                       chunk_id  \\\n",
              "0  6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e_chunk_0   \n",
              "1  6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e_chunk_1   \n",
              "2  4e290b0a-219e-09a3-c65f-f3c995c9f987_chunk_0   \n",
              "3  4e290b0a-219e-09a3-c65f-f3c995c9f987_chunk_1   \n",
              "4  2b0f1a6c-5f6a-fabc-7f0f-014323655c78_chunk_0   \n",
              "\n",
              "                                                text disease_category  \\\n",
              "0  She with multiple admissions for gastroparesis...         Diabetes   \n",
              "1  on the monitor at times, but is coming down wi...         Diabetes   \n",
              "2  nausea, vomiting, malaise Last night the patie...         Diabetes   \n",
              "3  is feeling better and reports the Ativan and T...         Diabetes   \n",
              "4  Polyuria, polydypsia, weight loss Male who has...         Diabetes   \n",
              "\n",
              "   disease_subtype                                        source_file  \\\n",
              "0  Type I Diabetes  /content/drive/MyDrive/mimic-iv-ext-direct-1.0...   \n",
              "1  Type I Diabetes  /content/drive/MyDrive/mimic-iv-ext-direct-1.0...   \n",
              "2  Type I Diabetes  /content/drive/MyDrive/mimic-iv-ext-direct-1.0...   \n",
              "3  Type I Diabetes  /content/drive/MyDrive/mimic-iv-ext-direct-1.0...   \n",
              "4  Type I Diabetes  /content/drive/MyDrive/mimic-iv-ext-direct-1.0...   \n",
              "\n",
              "   chunk_index  total_chunks  \\\n",
              "0            0             2   \n",
              "1            1             2   \n",
              "2            0             2   \n",
              "3            1             2   \n",
              "4            0             1   \n",
              "\n",
              "                                            metadata  \n",
              "0  {\"Type I diabetes$Intermedia_4\": {\"ICA antibod...  \n",
              "1  {\"Type I diabetes$Intermedia_4\": {\"ICA antibod...  \n",
              "2  {\"Type I Diabetes$Intermedia_4\": {\"ICA antibod...  \n",
              "3  {\"Type I Diabetes$Intermedia_4\": {\"ICA antibod...  \n",
              "4  {\"Type I diabetes$Intermedia_4\": {\"GADA antibo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b4af6d5-7caf-4849-9884-c08b93dbdc6c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>text</th>\n",
              "      <th>disease_category</th>\n",
              "      <th>disease_subtype</th>\n",
              "      <th>source_file</th>\n",
              "      <th>chunk_index</th>\n",
              "      <th>total_chunks</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e</td>\n",
              "      <td>6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e_chunk_0</td>\n",
              "      <td>She with multiple admissions for gastroparesis...</td>\n",
              "      <td>Diabetes</td>\n",
              "      <td>Type I Diabetes</td>\n",
              "      <td>/content/drive/MyDrive/mimic-iv-ext-direct-1.0...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>{\"Type I diabetes$Intermedia_4\": {\"ICA antibod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e</td>\n",
              "      <td>6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e_chunk_1</td>\n",
              "      <td>on the monitor at times, but is coming down wi...</td>\n",
              "      <td>Diabetes</td>\n",
              "      <td>Type I Diabetes</td>\n",
              "      <td>/content/drive/MyDrive/mimic-iv-ext-direct-1.0...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>{\"Type I diabetes$Intermedia_4\": {\"ICA antibod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4e290b0a-219e-09a3-c65f-f3c995c9f987</td>\n",
              "      <td>4e290b0a-219e-09a3-c65f-f3c995c9f987_chunk_0</td>\n",
              "      <td>nausea, vomiting, malaise Last night the patie...</td>\n",
              "      <td>Diabetes</td>\n",
              "      <td>Type I Diabetes</td>\n",
              "      <td>/content/drive/MyDrive/mimic-iv-ext-direct-1.0...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>{\"Type I Diabetes$Intermedia_4\": {\"ICA antibod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4e290b0a-219e-09a3-c65f-f3c995c9f987</td>\n",
              "      <td>4e290b0a-219e-09a3-c65f-f3c995c9f987_chunk_1</td>\n",
              "      <td>is feeling better and reports the Ativan and T...</td>\n",
              "      <td>Diabetes</td>\n",
              "      <td>Type I Diabetes</td>\n",
              "      <td>/content/drive/MyDrive/mimic-iv-ext-direct-1.0...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>{\"Type I Diabetes$Intermedia_4\": {\"ICA antibod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2b0f1a6c-5f6a-fabc-7f0f-014323655c78</td>\n",
              "      <td>2b0f1a6c-5f6a-fabc-7f0f-014323655c78_chunk_0</td>\n",
              "      <td>Polyuria, polydypsia, weight loss Male who has...</td>\n",
              "      <td>Diabetes</td>\n",
              "      <td>Type I Diabetes</td>\n",
              "      <td>/content/drive/MyDrive/mimic-iv-ext-direct-1.0...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>{\"Type I diabetes$Intermedia_4\": {\"GADA antibo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b4af6d5-7caf-4849-9884-c08b93dbdc6c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7b4af6d5-7caf-4849-9884-c08b93dbdc6c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7b4af6d5-7caf-4849-9884-c08b93dbdc6c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9230f491-653a-4f31-97da-ec3208443501\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9230f491-653a-4f31-97da-ec3208443501')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9230f491-653a-4f31-97da-ec3208443501 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"globals()['processed_df'] = df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e\",\n          \"4e290b0a-219e-09a3-c65f-f3c995c9f987\",\n          \"2b0f1a6c-5f6a-fabc-7f0f-014323655c78\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e_chunk_1\",\n          \"2b0f1a6c-5f6a-fabc-7f0f-014323655c78_chunk_0\",\n          \"4e290b0a-219e-09a3-c65f-f3c995c9f987_chunk_0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"on the monitor at times, but is coming down with ivf. her bp is high but pt has not been taking her meds, gave her am dose of amlodipine today. \\ufeff +Severe anxiety/panic attacks +Depression with psychotic features, currently seeing doctor for Psychiatric care +Esophagitis / H. Pylori s/p 2-weektriple therapy +Grade I esophageal varices seen on scope,negative liver ultrasound, normal LFTs, hep panel negative +Hyperlipidemia +S/P - lower back pain since then. +S/P ex-lap +G2P1Ab1, s/p miscarriage, s/p C-sectionin +Genital Herpes +Left tibial plateau fracture s/p ORIF +CKD stage II-III (b/l creatinine 1.5-2.0) Diabetes in her grandmother and asthma. Physical Exam: ADMISSION Vitals- 97.9 100RA BS 106 General- Alert, oriented, no acute distress HEENT- Sclera anicteric, MMM, oropharynx clear Neck- supple, JVP not elevated, no LAD Lungs- Clear to auscultation bilaterally, no wheezes, rales, ronchi CV- Regular rate and rhythm, normal S1 + S2, no murmurs, rubs, gallops Abdomen- soft, non-tender, non-distended, hypoactive bowel sounds present, no rebound tenderness or guarding, no organomegaly GU- no foley Ext- warm, well perfused, 2+ pulses, no clubbing, cyanosis or edema Neuro- CNs2-12 intact, motor function grossly normal ACTIVE ISSUES # S/P FALL: Likely secondary to hypoglycemia. Given significant and longstanding poor blood sugar control, possible patient has significant autonomic dysfunction. EKG with shortened PR, but no other signs of WPW. Had a fall on prior admission with sutures, which were removed on this admission. \\ufeff # HTN: Patient with significant diastolic pressure >100 on admission. On amlodipine and lisinopril at home which patient has been complaint. Has missed her BP meds on day of admission.Prior symptomatic hypotension so would be careful about giving too much BP meds. Pressures possibly labile as above from autonomic dysfunction. \\ufeff INACTIVE ISSUE # DEPRESSION WITH PSYCHOTIC FEATURES: Seeing doctor for Psychiatric care. Continued Risperdone. \\ufeff TRANSITIONAL ISSUES # Go over insulin plan for poor PO intake with patient and ensure that she understands \\ufeff ADMISSION \\ufeff ___ 01:00PM BLOOD WBC-10.3 RBC-3.29* Hgb-9.4* Hct-29.1* MCV-88 MCH-28.5 MCHC-32.2 RDW-13.2 ___ 01:00PM BLOOD Glucose-263* UreaN-18 Creat-1.6* Na-135 K-3.8 Cl-100 HCO3-26 AnGap-13 ___ 01:00PM BLOOD Albumin-3.5 Calcium-9.1 Phos-2.1*# Mg-1.5* ___ 01:07PM BLOOD Lactate-0.7 ___ 01:07PM ICA-(+)*\",\n          \"Polyuria, polydypsia, weight loss Male who has been experiencing dry mouth, polyuria (>20 times/day)and polydypsia with associated 15 lb weight loss over the past 3 weeks. Also with occasional sweats, nausea and loss of appetite. No fevers, recent infections or new medications. One day PTA he went to his center. He received 1L NS. . In the ED, vitals were: 97.1 128/68 60 18 97% RA. His initial fingerstick was 568 with glucose and ketones in urine. He was called who recommended 10u of regular insulin. He given another 1L NS with 40meq KCL. Repeat chemistry showed glucose 365. AG = 14. He was admitted to medicine for further management. . ROS: No fevers, H/A, CP or abdominal pain. No blurry vision. \\ufeff 1. Concussion 2. Muscle strain in neck 3. Epistaxis - s/p cauterization x 2 Grandfather with DM. Otherwise NC Vitals: 97.4 96.9 122/78 54 18 98% RA. ___ 345, 321 GEN: AAOx3, well appearing, sitting in bed, NAD HEENT: PERRL, EOMI. MMM. Oropharynx without lesions or exudate. \\ufeff NECK: Supple, no cervical or supraclavicular LAD LUNGS: CTAB. Normal respiratory effort. HEART: RRR, no m/r/g ABD: NABS, soft, non-tender, non-distended, no masses EXT: WWP, good DP pulses bilaterally, no c/c/e NEURO: CN ___ grossly intact bilaterally. ___ MS in bilateral upper and lower extremities. 2+ biceps, patellar DTRs. ___ 01:10AM WBC-7.7 RBC-4.91 HGB-15.0 HCT-40.2 MCV-82 MCH-30.6 MCHC-37.4* RDW-12.3 ___ 01:10AM GLUCOSE-568* UREA N-27* CREAT-1.1 SODIUM-130* POTASSIUM-4.0 CHLORIDE-92* TOTAL CO2-24 ANION GAP-18 ___ 02:00AM URINE BLOOD-NEG NITRITE-NEG PROTEIN-NEG GLUCOSE-1000 KETONE-50 BILIRUBIN-NEG UROBILNGN-NEG PH-5.0 LEUK-NEG ___ 02:00AM GADA-(+)*\",\n          \"nausea, vomiting, malaise Last night the patient became nauseous, started vomiting, had palpitations and felt dyspneic. She had just eaten some tuna tartare. Noted that her BG was running in 200s, and it typically runs in the 150-180 range. Also endorsed lightheadedness. This brought her to the ED. In ED initial VS: 98.2 110 118/69 20 99% RA Exam: not documented in dashboard Labs significant for: glu 203, AG 29, bicarb 12, pH 7.27 initially Patient was given:-2L NS then started on NS at 250 cc/hr (unsure how much fluid in total she rec'd) -started on insulin gtt -Zofran, Ativan, Toradol for nausea and headache Imaging notable for: CXR unremarkable \\ufeff She was seen in the ED and kept in the CDU overnight. Her AG closed in the ED, after which her insulin gtt was dc'd. Per patient, the plan had been to stop the insulin gtt and have her insulin pump turned back on. However, she went about 4 hours without getting any insulin, and her gap opened back up. She then rec'd a dose of SC Lispro 4U, was restarted on an insulin gtt and admitted to the ICU. \\ufeff VS prior to transfer: 98.4 96 95/56 18 98% RA On arrival to the MICU, she is feeling better and reports the Ativan and Toradol have helped her nausea and headache. Still feeling SOB if she stands up. Doesn't feel comfortable eating yet but maybe in a few hours. Does not feel like she has been sick lately. No change in meds, diet or alcohol use. Also having vaginal itching that feels like prior yeast infections. \\ufeff REVIEW OF SYSTEMS: per HPI. otherwise denies chest pain, fever, cough, dysuria \\ufeff +CARPAL TUNNEL SURGERY +RADIUS FACTURE +ankle fracture per chart review: sister with type one diabetes and papillary thyroid carcinoma Admission PHYSICAL EXAM: VITALS: please see Metavision.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disease_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Diabetes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disease_subtype\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Type I Diabetes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"/content/drive/MyDrive/mimic-iv-ext-direct-1.0.0/mimic-iv-ext-direct-1.0.0/Finished/Diabetes/Type I Diabetes/17517983-DS-78.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_chunks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"{\\\"Type I diabetes$Intermedia_4\\\": {\\\"ICA antibodies are positive in most patients with type I diabetes.$Cause_1\\\": {\\\"ICA-(+)*$Input6\\\": {}}, \\\"Diabetes$Intermedia_3\\\": {\\\"Abnormal blood glucose is a diagnostic criteria of diabetes.$Cause_1\\\": {\\\"BLOOD Glucose-263$Input6\\\": {}}, \\\"Suspected Diabetes$Intermedia_2\\\": {\\\"The abnormal blood glucose in the presence of classic sympotoms of hyperglycemia is a diagnostic criteria of diabetes.$Cause_1\\\": {\\\"hyperglycemia >500$Input2\\\": {}}, \\\"FSG is a symptom of diabetes.$Cause_1\\\": {\\\"FSG of 50$Input2\\\": {}}, \\\"CKD is a symptom of diabetes.$Cause_1\\\": {\\\"+CKD stage II-III (b/l creatinine 1.5-2.0)$Input3\\\": {}}, \\\"Family history is a big risk factor of diabetes.$Cause_1\\\": {\\\"Diabetes in her grandmother$Input4\\\": {}}}}}, \\\"input1\\\": \\\"Hypoglycemia\\\\n\\\", \\\"input2\\\": \\\"She with multiple admissions for gastroparesis over last year with ED eval 3 days prior for hyperglycemia >500 treated with IV fluids now presents after being found down at home with sugar of 50. At that point was given glucagon. \\\\n\\ufeff\\\\nHas significant number of prior admissions for nausea/vomitting attributed to gastroparesis--per PCP usually episodes are set off by life stressors. Current stressors include her mother moving to an apartment farther away with inconvenient transportation. Her year old son is having difficulty in school due to ADHD diagnosis. Lives alone but is very close to her mother. Previously has had self-motivation to take charge of own care, but recently has required increasing degree of intervention from Social Worker. Seen a week ago and per PCP had more distant affect.\\\\n\\ufeff\\\\nThis AM took her insulin 70/30 at 28 units as prescribed and proceeded to get ready fro appointment to have stitches removed that had been placed after last fall. Felt nauseated and did not eat--otherwise no symptoms of hypoglycemia.  No increase in polyuria, no frank vomitting, no diarrhea. No prodrome before she blacked out. Found by sister who called for ambulance. EMS found her to have FSG of 50. At that point was given glucagon. Currently complains of R sided chest wall pain and R flank pain, unchanged from pain she was experiencing upon presentation to the ED 3 days prior, CXR and rib films at that time negative. \\\\n\\ufeff\\\\nIn the ED, initial vital signs were 97.4 110 143/93 18 98% \\\\nPatient was given.  Pt having nausea and vomitting, so received 4mg iv zofran, 4mg po zofran, 10mg reglan and 1mg iv ativan. She received 0.5mg iv dilaudid for her chronic pain. Was tachy to 120's, on the monitor at times, but is coming down with ivf. her bp is high but pt has not been taking her meds, gave her am dose of amlodipine today. \\\\n\\ufeff\\\\n\\\", \\\"input3\\\": \\\"+Severe anxiety/panic attacks\\\\n+Depression with psychotic features, currently seeing doctor for Psychiatric care\\\\n+Esophagitis / H. Pylori s/p 2-weektriple therapy\\\\n+Grade I esophageal varices seen on scope,negative liver ultrasound, normal LFTs, hep panel negative\\\\n+Hyperlipidemia\\\\n+S/P - lower back pain since then.\\\\n+S/P  ex-lap\\\\n+G2P1Ab1, s/p miscarriage, s/p C-sectionin\\\\n+Genital Herpes\\\\n+Left tibial plateau fracture s/p ORIF\\\\n+CKD stage II-III (b/l creatinine 1.5-2.0)\\\\n\\\", \\\"input4\\\": \\\"Diabetes in her grandmother and asthma.\\\\n\\\", \\\"input5\\\": \\\"Physical Exam:\\\\nADMISSION\\\\nVitals- 97.9 100RA BS 106\\\\nGeneral- Alert, oriented, no acute distress  \\\\nHEENT- Sclera anicteric, MMM, oropharynx clear  \\\\nNeck- supple, JVP not elevated, no LAD  \\\\nLungs- Clear to auscultation bilaterally, no wheezes, rales, ronchi  \\\\nCV- Regular rate and rhythm, normal S1 + S2, no murmurs, rubs, gallops  \\\\nAbdomen- soft, non-tender, non-distended, hypoactive bowel sounds present, no rebound tenderness or guarding, no organomegaly  \\\\nGU- no foley  \\\\nExt- warm, well perfused, 2+ pulses, no clubbing, cyanosis or edema\\\\nNeuro- CNs2-12 intact, motor function grossly normal  \\\\n\\\\nACTIVE ISSUES\\\\n# S/P FALL: Likely secondary to hypoglycemia. Given significant and longstanding poor blood sugar control, possible patient has significant autonomic dysfunction. EKG with shortened PR, but no other signs of WPW. Had a fall on prior admission with sutures, which were removed on this admission.\\\\n\\ufeff\\\\n# HTN: Patient with significant diastolic pressure >100 on admission. On amlodipine and lisinopril at home which patient has been complaint. Has missed her BP meds on day of admission.Prior symptomatic hypotension so would be careful about giving too much BP meds. Pressures possibly labile as above from autonomic dysfunction.\\\\n\\ufeff\\\\nINACTIVE ISSUE\\\\n# DEPRESSION WITH PSYCHOTIC FEATURES: Seeing doctor for Psychiatric care. Continued Risperdone.\\\\n\\ufeff\\\\nTRANSITIONAL ISSUES\\\\n# Go over insulin plan for poor PO intake with patient and ensure that she understands\\\\n\\ufeff\\\\n\\\", \\\"input6\\\": \\\"ADMISSION\\\\n\\ufeff\\\\n___ 01:00PM BLOOD WBC-10.3 RBC-3.29* Hgb-9.4* Hct-29.1* MCV-88 MCH-28.5 MCHC-32.2 RDW-13.2\\\\n___ 01:00PM BLOOD Glucose-263* UreaN-18 Creat-1.6* Na-135 K-3.8 Cl-100 HCO3-26 AnGap-13\\\\n___ 01:00PM BLOOD Albumin-3.5 Calcium-9.1 Phos-2.1*# Mg-1.5*\\\\n___ 01:07PM BLOOD Lactate-0.7\\\\n___ 01:07PM ICA-(+)*\\\\n\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“ˆ Dataset Statistics:\n",
            "  Total chunks: 934\n",
            "  Unique documents: 510\n",
            "  Disease categories: 25\n",
            "  Average chunk length: 2024 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 7: EMBEDDING GENERATOR WITH CACHING"
      ],
      "metadata": {
        "id": "7f1H9RoCDmJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingGenerator:\n",
        "    \"\"\"Generate E5 embeddings with Google Drive caching.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config, device: str = None):\n",
        "        self.config = config\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load E5 embedding model with caching.\"\"\"\n",
        "        print(f\"ðŸ“¥ Loading embedding model: {self.config.EMBEDDING_MODEL}\")\n",
        "        print(f\"   Device: {self.device}\")\n",
        "\n",
        "        # Models cache to Google Drive for faster loading\n",
        "        cache_dir = str(self.config.MODELS_CACHE)\n",
        "\n",
        "        self.model = SentenceTransformer(\n",
        "            self.config.EMBEDDING_MODEL,\n",
        "            device=self.device,\n",
        "            cache_folder=cache_dir\n",
        "        )\n",
        "        self.model.eval()\n",
        "\n",
        "        print(\"âœ“ Embedding model loaded\")\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            mem = get_gpu_memory()\n",
        "            print(f\"  GPU Memory: {mem['allocated']:.2f}GB used / {mem['total']:.2f}GB total\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
        "        \"\"\"Generate embeddings with progress bar and adaptive batching.\"\"\"\n",
        "        print(f\"\\nðŸ”¢ Generating embeddings for {len(texts)} documents...\")\n",
        "        print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "        embeddings = []\n",
        "        failed_batches = []\n",
        "\n",
        "        pbar = tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\", unit=\"batch\")\n",
        "\n",
        "        for i in pbar:\n",
        "            batch = texts[i:i + batch_size]\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    batch_embeddings = self.model.encode(\n",
        "                        batch,\n",
        "                        normalize_embeddings=True,\n",
        "                        show_progress_bar=False,\n",
        "                        convert_to_numpy=True,\n",
        "                        batch_size=batch_size\n",
        "                    )\n",
        "\n",
        "                embeddings.append(batch_embeddings)\n",
        "\n",
        "                # Update progress\n",
        "                pbar.set_postfix({\n",
        "                    'completed': len(embeddings) * batch_size,\n",
        "                    'total': len(texts)\n",
        "                })\n",
        "\n",
        "                # Periodic memory cleanup\n",
        "                if self.device == \"cuda\" and i % (batch_size * 10) == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e).lower():\n",
        "                    print(f\"\\nâš  OOM at batch {i}. Reducing batch size...\")\n",
        "                    cleanup_memory()\n",
        "                    batch_size = max(1, batch_size // 2)\n",
        "                    print(f\"   New batch size: {batch_size}\")\n",
        "\n",
        "                    # Retry current batch with smaller size\n",
        "                    return self.generate_embeddings(texts[i:], batch_size)\n",
        "                else:\n",
        "                    print(f\"\\nâŒ Error in batch {i}: {str(e)}\")\n",
        "                    failed_batches.append(i)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        if failed_batches:\n",
        "            print(f\"âš  Warning: {len(failed_batches)} batches failed\")\n",
        "\n",
        "        final_embeddings = np.vstack(embeddings)\n",
        "        print(f\"âœ“ Generated {len(final_embeddings)} embeddings\")\n",
        "        print(f\"  Shape: {final_embeddings.shape}\")\n",
        "        print(f\"  Size: {final_embeddings.nbytes / 1e6:.2f} MB\")\n",
        "\n",
        "        return final_embeddings\n",
        "\n",
        "    def save_embeddings(self, embeddings: np.ndarray) -> Path:\n",
        "        \"\"\"Save embeddings to Google Drive.\"\"\"\n",
        "        print(\"\\nðŸ’¾ Saving embeddings to Google Drive...\")\n",
        "        output_path = self.config.EMBEDDINGS_PATH\n",
        "        np.save(output_path, embeddings)\n",
        "        print(f\"âœ“ Embeddings saved: {output_path}\")\n",
        "        print(f\"  Size: {output_path.stat().st_size / 1e6:.2f} MB\")\n",
        "        return output_path\n",
        "\n",
        "    def load_embeddings(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load embeddings from Google Drive if exists.\"\"\"\n",
        "        if self.config.EMBEDDINGS_PATH.exists():\n",
        "            print(\"ðŸ“‚ Loading cached embeddings from Google Drive...\")\n",
        "            embeddings = np.load(self.config.EMBEDDINGS_PATH)\n",
        "            print(f\"âœ“ Loaded embeddings: {embeddings.shape}\")\n",
        "            return embeddings\n",
        "        return None\n",
        "\n",
        "    def create_chromadb_collection(self, df: pd.DataFrame, embeddings: np.ndarray):\n",
        "        \"\"\"Create and populate ChromaDB collection with progress tracking.\"\"\"\n",
        "        print(\"\\nðŸ—„ï¸ Setting up ChromaDB...\")\n",
        "\n",
        "        # Initialize ChromaDB with Google Drive persistence\n",
        "        chroma_client = chromadb.PersistentClient(\n",
        "            path=str(self.config.CHROMA_DB_PATH),\n",
        "            settings=Settings(anonymized_telemetry=False, allow_reset=True)\n",
        "        )\n",
        "\n",
        "        # Delete existing collection if exists\n",
        "        try:\n",
        "            chroma_client.delete_collection(name=\"clinical_notes\")\n",
        "            print(\"  Deleted existing collection\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Create new collection\n",
        "        collection = chroma_client.create_collection(\n",
        "            name=\"clinical_notes\",\n",
        "            metadata={\n",
        "                \"hnsw:space\": \"cosine\",\n",
        "                \"description\": \"MIMIC-IV clinical notes with E5 embeddings\",\n",
        "                \"created_at\": datetime.now().isoformat()\n",
        "            }\n",
        "        )\n",
        "        print(\"âœ“ Collection created\")\n",
        "\n",
        "        # Prepare data\n",
        "        print(\"\\nðŸ“‹ Preparing data for ChromaDB...\")\n",
        "        chroma_ids = df['chunk_id'].tolist()\n",
        "        chroma_documents = df['text'].tolist()\n",
        "        chroma_embeddings = embeddings.tolist()\n",
        "\n",
        "        chroma_metadatas = []\n",
        "        for idx, row in df.iterrows():\n",
        "            metadata = {\n",
        "                'doc_id': str(row['doc_id']),\n",
        "                'disease_category': str(row['disease_category']),\n",
        "                'disease_subtype': str(row['disease_subtype']),\n",
        "                'chunk_index': int(row['chunk_index']),\n",
        "                'total_chunks': int(row['total_chunks']),\n",
        "                'source_file': str(row['source_file'])\n",
        "            }\n",
        "            chroma_metadatas.append(metadata)\n",
        "\n",
        "        print(f\"  Documents: {len(chroma_ids)}\")\n",
        "        print(f\"  Embeddings: {len(chroma_embeddings)}\")\n",
        "\n",
        "        # Add in batches with progress bar\n",
        "        batch_size = 500\n",
        "        print(f\"\\nðŸ“¤ Adding documents to ChromaDB (batch size: {batch_size})...\")\n",
        "\n",
        "        pbar = tqdm(range(0, len(chroma_ids), batch_size), desc=\"Adding batches\", unit=\"batch\")\n",
        "\n",
        "        for i in pbar:\n",
        "            batch_end = min(i + batch_size, len(chroma_ids))\n",
        "\n",
        "            collection.add(\n",
        "                ids=chroma_ids[i:batch_end],\n",
        "                documents=chroma_documents[i:batch_end],\n",
        "                embeddings=chroma_embeddings[i:batch_end],\n",
        "                metadatas=chroma_metadatas[i:batch_end]\n",
        "            )\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'added': batch_end,\n",
        "                'total': len(chroma_ids)\n",
        "            })\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        print(f\"\\nâœ“ ChromaDB collection populated\")\n",
        "        print(f\"  Total documents: {collection.count()}\")\n",
        "        print(f\"  Storage: {self.config.CHROMA_DB_PATH}\")\n",
        "\n",
        "        return collection\n",
        "\n",
        "print(\"âœ“ EmbeddingGenerator class loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St2Af39HDo8u",
        "outputId": "459e99c3-1631-4e3e-a83f-a50123695a4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ EmbeddingGenerator class loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 8: RUN EMBEDDING GENERATION (Skip if cached)"
      ],
      "metadata": {
        "id": "DIfIYnYYDrq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 2: EMBEDDING GENERATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize embedder\n",
        "embedder = EmbeddingGenerator(config, device)\n",
        "\n",
        "# Try to load cached embeddings\n",
        "embeddings = embedder.load_embeddings()\n",
        "\n",
        "if embeddings is None:\n",
        "    print(\"\\nâš  No cached embeddings found. Generating new embeddings...\")\n",
        "    print(\"â± This will take 5-15 minutes depending on dataset size\\n\")\n",
        "\n",
        "    # Load model\n",
        "    embedder.load_model()\n",
        "\n",
        "    # Prepare documents with E5 prefix\n",
        "    print(\"\\nðŸ“ Preparing documents with E5 passage prefix...\")\n",
        "    documents_with_prefix = [\n",
        "        f\"{config.PASSAGE_PREFIX}{text}\"\n",
        "        for text in processed_df['text'].tolist()\n",
        "    ]\n",
        "    print(f\"âœ“ Prepared {len(documents_with_prefix)} documents\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate_embeddings(documents_with_prefix, batch_size=32)\n",
        "\n",
        "    # Save to Google Drive\n",
        "    embedder.save_embeddings(embeddings)\n",
        "\n",
        "    print(\"\\nâœ“ Embeddings generated and saved to Google Drive!\")\n",
        "\n",
        "    # Clean up model from memory\n",
        "    del embedder.model\n",
        "    cleanup_memory()\n",
        "else:\n",
        "    print(\"\\nâœ“ Using cached embeddings from Google Drive!\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Embeddings Summary:\")\n",
        "print(f\"  Shape: {embeddings.shape}\")\n",
        "print(f\"  Dtype: {embeddings.dtype}\")\n",
        "print(f\"  Memory: {embeddings.nbytes / 1e6:.2f} MB\")\n",
        "\n",
        "# Save reference for later cells\n",
        "globals()['embeddings'] = embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oCPd-KeDs-c",
        "outputId": "f80747b5-c051-45ed-f92b-058b5dbd10b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 2: EMBEDDING GENERATION\n",
            "================================================================================\n",
            "ðŸ“‚ Loading cached embeddings from Google Drive...\n",
            "âœ“ Loaded embeddings: (934, 384)\n",
            "\n",
            "âœ“ Using cached embeddings from Google Drive!\n",
            "\n",
            "ðŸ“Š Embeddings Summary:\n",
            "  Shape: (934, 384)\n",
            "  Dtype: float32\n",
            "  Memory: 1.43 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 9: SETUP CHROMADB VECTOR DATABASE (Skip if exists)"
      ],
      "metadata": {
        "id": "Yr7MFt_ADu0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 3: CHROMADB VECTOR DATABASE SETUP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check if ChromaDB already exists\n",
        "chroma_db_exists = (config.CHROMA_DB_PATH / \"chroma.sqlite3\").exists()\n",
        "\n",
        "if chroma_db_exists:\n",
        "    print(\"ðŸ“‚ ChromaDB already exists in Google Drive\")\n",
        "    print(f\"   Path: {config.CHROMA_DB_PATH}\")\n",
        "\n",
        "    # Connect to existing database\n",
        "    chroma_client = chromadb.PersistentClient(\n",
        "        path=str(config.CHROMA_DB_PATH),\n",
        "        settings=Settings(anonymized_telemetry=False)\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        collection = chroma_client.get_collection(name=\"clinical_notes\")\n",
        "        print(f\"âœ“ Connected to existing collection\")\n",
        "        print(f\"  Documents: {collection.count()}\")\n",
        "\n",
        "        # Get a sample to verify\n",
        "        sample = collection.peek(limit=1)\n",
        "        print(f\"  Sample metadata: {sample['metadatas'][0] if sample['metadatas'] else 'None'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš  Error accessing collection: {e}\")\n",
        "        print(\"  Will recreate collection...\")\n",
        "        chroma_db_exists = False\n",
        "\n",
        "if not chroma_db_exists:\n",
        "    print(\"\\nâš  ChromaDB not found. Creating new collection...\")\n",
        "    print(\"â± This will take 3-10 minutes\\n\")\n",
        "\n",
        "    # Reinitialize embedder (without model, just for DB creation)\n",
        "    embedder_for_db = EmbeddingGenerator(config, device)\n",
        "\n",
        "    # Create ChromaDB collection\n",
        "    collection = embedder_for_db.create_chromadb_collection(processed_df, embeddings)\n",
        "\n",
        "    print(\"\\nâœ“ ChromaDB collection created and saved to Google Drive!\")\n",
        "\n",
        "print(\"\\nâœ… ChromaDB Ready!\")\n",
        "print(f\"   Location: {config.CHROMA_DB_PATH}\")\n",
        "print(f\"   Documents: {collection.count()}\")\n",
        "\n",
        "# Save reference for later cells\n",
        "globals()['chroma_collection'] = collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGWdxNgdDxbs",
        "outputId": "d8685054-0517-4cf1-d365-d3417d494d1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 3: CHROMADB VECTOR DATABASE SETUP\n",
            "================================================================================\n",
            "ðŸ“‚ ChromaDB already exists in Google Drive\n",
            "   Path: /content/drive/MyDrive/Clinical_RAG_System/chroma_db\n",
            "âœ“ Connected to existing collection\n",
            "  Documents: 934\n",
            "  Sample metadata: {'source_file': '/content/drive/MyDrive/mimic-iv-ext-direct-1.0.0/mimic-iv-ext-direct-1.0.0/Finished/Diabetes/Type I Diabetes/17517983-DS-78.json', 'chunk_index': 0, 'disease_category': 'Diabetes', 'total_chunks': 2, 'doc_id': '6ae055d0-ca1e-a4b3-e7f0-d0c0d15f312e', 'disease_subtype': 'Type I Diabetes'}\n",
            "\n",
            "âœ… ChromaDB Ready!\n",
            "   Location: /content/drive/MyDrive/Clinical_RAG_System/chroma_db\n",
            "   Documents: 934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 10: TEST CHROMADB QUERIES"
      ],
      "metadata": {
        "id": "jzNMG-ljDz8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TESTING CHROMADB QUERIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load ChromaDB if not already loaded\n",
        "if 'chroma_collection' not in globals():\n",
        "    chroma_client = chromadb.PersistentClient(\n",
        "        path=str(config.CHROMA_DB_PATH),\n",
        "        settings=Settings(anonymized_telemetry=False)\n",
        "    )\n",
        "    chroma_collection = chroma_client.get_collection(name=\"clinical_notes\")\n",
        "\n",
        "# Load embedding model for queries\n",
        "print(\"\\nðŸ“¥ Loading embedding model for queries...\")\n",
        "query_model = SentenceTransformer(\n",
        "    config.EMBEDDING_MODEL,\n",
        "    device=device,\n",
        "    cache_folder=str(config.MODELS_CACHE)\n",
        ")\n",
        "query_model.eval()\n",
        "print(\"âœ“ Model loaded\")\n",
        "\n",
        "def test_query(query_text: str, top_k: int = 3):\n",
        "    \"\"\"Test a query against ChromaDB.\"\"\"\n",
        "    print(f\"\\nðŸ” Query: '{query_text}'\")\n",
        "\n",
        "    # Encode query with E5 prefix\n",
        "    query_with_prefix = f\"{config.QUERY_PREFIX}{query_text}\"\n",
        "    query_embedding = query_model.encode(\n",
        "        query_with_prefix,\n",
        "        normalize_embeddings=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "\n",
        "    # Search ChromaDB\n",
        "    results = chroma_collection.query(\n",
        "        query_embeddings=[query_embedding.tolist()],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nðŸ“„ Top {top_k} Results:\")\n",
        "    for i, (doc, metadata, distance) in enumerate(zip(\n",
        "        results['documents'][0],\n",
        "        results['metadatas'][0],\n",
        "        results['distances'][0]\n",
        "    ), 1):\n",
        "        similarity = 1 - distance\n",
        "        print(f\"\\n--- Result {i} (Similarity: {similarity:.3f}) ---\")\n",
        "        print(f\"Category: {metadata.get('disease_category', 'N/A')}\")\n",
        "        print(f\"Subtype: {metadata.get('disease_subtype', 'N/A')}\")\n",
        "        print(f\"Text preview: {doc[:200]}...\")\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What are the symptoms of pneumonia?\",\n",
        "    \"Treatment options for heart failure\",\n",
        "    \"Diagnosis of diabetes mellitus\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING TEST QUERIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for query in test_queries:\n",
        "    test_query(query, top_k=2)\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "print(\"\\nâœ“ ChromaDB testing complete!\")\n",
        "\n",
        "# Clean up test model\n",
        "del query_model\n",
        "cleanup_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7fa5fa59e83443e39ee3f117369c7158",
            "2d956f81922e4db2a6511d6654a68892",
            "4e033e1c9a1f439a925dcb7a53dd0d2d",
            "ad2821b4414047f3915bda2dab53a6c9",
            "fa8dfe15906442389c5c59464eb45101",
            "064b03538d8d4c5483c7119dbb8808a1",
            "25b081b920434767871c67ee75d65c72",
            "c017fc45655e4a6b88f2c34ade59bb78",
            "d2119108db5b45de96c2cd2281665cec",
            "4120cabfcf6a40369b9d43d979a17cca",
            "bc352bb620344f08a668f91e2d52a2cb"
          ]
        },
        "id": "IaxWPmwXD1TV",
        "outputId": "5f675315-4220-4f5a-bfa9-ee135c4af36f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING CHROMADB QUERIES\n",
            "================================================================================\n",
            "\n",
            "ðŸ“¥ Loading embedding model for queries...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fa5fa59e83443e39ee3f117369c7158"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model loaded\n",
            "\n",
            "================================================================================\n",
            "RUNNING TEST QUERIES\n",
            "================================================================================\n",
            "\n",
            "ðŸ” Query: 'What are the symptoms of pneumonia?'\n",
            "\n",
            "ðŸ“„ Top 2 Results:\n",
            "\n",
            "--- Result 1 (Similarity: 0.874) ---\n",
            "Category: Pneumonia\n",
            "Subtype: Bacterial Pneumonia\n",
            "Text preview: Her symptoms began approximately 10 days ago with a mild headache, muscle and joint pain, and a runny nose with clear discharge. Her condition temporarily improved after taking Tylenol, but she soon e...\n",
            "\n",
            "--- Result 2 (Similarity: 0.870) ---\n",
            "Category: Pneumonia\n",
            "Subtype: Bacterial Pneumonia\n",
            "Text preview: On ___, he developed a nonproductive cough, which was progressively worsened. He denies any SOB. He felt very weak on ___. He was diagnosed with pneumonia in the ED yesterday, sent home on zpack. He d...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸ” Query: 'Treatment options for heart failure'\n",
            "\n",
            "ðŸ“„ Top 2 Results:\n",
            "\n",
            "--- Result 1 (Similarity: 0.856) ---\n",
            "Category: Atrial Fibrillation\n",
            "Subtype: Persistent Atrial Fibrillation\n",
            "Text preview: This 56 year old woman. She was started on Propafenone and did well until when she developed recurrent palpitations. She was found to be in recurrent atrial fibrillation. She has undergone two CV's. S...\n",
            "\n",
            "--- Result 2 (Similarity: 0.854) ---\n",
            "Category: Heart Failure\n",
            "Subtype: 18834270-DS-10.json\n",
            "Text preview: He is a 80 year old M w/ PMH HTN who presents with progressive SOB over the past week. No fever or chills, no chest pain. No leg swelling; no pain with breathing. Patient reports a history of hyperten...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸ” Query: 'Diagnosis of diabetes mellitus'\n",
            "\n",
            "ðŸ“„ Top 2 Results:\n",
            "\n",
            "--- Result 1 (Similarity: 0.877) ---\n",
            "Category: Acute Coronary Syndrome\n",
            "Subtype: STEMI\n",
            "Text preview: 68 y/o man with history of moderate mixed hyperlipidemia, recently diagnosed type II diabetes, presented to the emergency department today with severe chest pain. He has a history of similar pain for ...\n",
            "\n",
            "--- Result 2 (Similarity: 0.871) ---\n",
            "Category: Diabetes\n",
            "Subtype: Type II Diabetes\n",
            "Text preview: She is a woman with a history of obesity, anxiety/depression, GERD who was sent from her PCP's office for new diagnosis nausea/vomiting. She notes 2 weeks of polydipsia, polyuria, blurry vision, nause...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "âœ“ ChromaDB testing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 11: RAG PIPELINE WITH MISTRAL-7B"
      ],
      "metadata": {
        "id": "Wz2qnX4bD4UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline with retrieval and generation.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Models (loaded on demand)\n",
        "        self.embedding_model = None\n",
        "        self.generation_model = None\n",
        "        self.tokenizer = None\n",
        "        self.collection = None\n",
        "\n",
        "        self.stats = {\n",
        "            'queries_processed': 0,\n",
        "            'total_retrieval_time': 0,\n",
        "            'total_generation_time': 0,\n",
        "            'total_tokens_generated': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "    def load_models(self, load_generation_model: bool = True):\n",
        "        \"\"\"Load all required models with progress tracking.\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"LOADING RAG PIPELINE MODELS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # 1. Load embedding model\n",
        "        print(\"\\n[1/3] ðŸ“¥ Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer(\n",
        "            self.config.EMBEDDING_MODEL,\n",
        "            device=self.device,\n",
        "            cache_folder=str(self.config.MODELS_CACHE)\n",
        "        )\n",
        "        self.embedding_model.eval()\n",
        "        print(\"      âœ“ Embedding model loaded\")\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            mem = get_gpu_memory()\n",
        "            print(f\"      GPU Memory: {mem['allocated']:.2f}GB / {mem['total']:.2f}GB\")\n",
        "\n",
        "        # 2. Load ChromaDB\n",
        "        print(\"\\n[2/3] ðŸ“‚ Loading ChromaDB collection...\")\n",
        "        chroma_client = chromadb.PersistentClient(\n",
        "            path=str(self.config.CHROMA_DB_PATH),\n",
        "            settings=Settings(anonymized_telemetry=False)\n",
        "        )\n",
        "        self.collection = chroma_client.get_collection(name=\"clinical_notes\")\n",
        "        print(f\"      âœ“ Collection loaded ({self.collection.count()} documents)\")\n",
        "\n",
        "        # 3. Load generation model (optional, heavy)\n",
        "        if load_generation_model:\n",
        "            print(\"\\n[3/3] ðŸ¤– Loading Mistral-7B (4-bit quantization)...\")\n",
        "            print(\"      â± This may take 2-5 minutes...\")\n",
        "\n",
        "            # Configure 4-bit quantization\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "            # Load tokenizer\n",
        "            print(\"      Loading tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.config.GENERATION_MODEL,\n",
        "                cache_dir=str(self.config.MODELS_CACHE)\n",
        "            )\n",
        "\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "            print(\"      âœ“ Tokenizer loaded\")\n",
        "\n",
        "            # Load model\n",
        "            print(\"      Loading Mistral-7B model...\")\n",
        "            self.generation_model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.config.GENERATION_MODEL,\n",
        "                quantization_config=bnb_config,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=True,\n",
        "                cache_dir=str(self.config.MODELS_CACHE),\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            print(\"      âœ“ Mistral-7B loaded\")\n",
        "\n",
        "            if self.device == \"cuda\":\n",
        "                mem = get_gpu_memory()\n",
        "                print(f\"      GPU Memory: {mem['allocated']:.2f}GB / {mem['total']:.2f}GB\")\n",
        "        else:\n",
        "            print(\"\\n[3/3] â­ï¸  Skipping generation model (retrieval-only mode)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"âœ… ALL MODELS LOADED SUCCESSFULLY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def encode_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Encode query with E5 prefix.\"\"\"\n",
        "        prefixed_query = f\"{self.config.QUERY_PREFIX}{query}\"\n",
        "        with torch.no_grad():\n",
        "            embedding = self.embedding_model.encode(\n",
        "                prefixed_query,\n",
        "                normalize_embeddings=True,\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "        return embedding\n",
        "\n",
        "    def retrieve_documents(self, query: str, top_k: int = 5,\n",
        "                          filters: Optional[Dict] = None) -> Tuple[List[Dict], float]:\n",
        "        \"\"\"Retrieve relevant documents from ChromaDB.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        query_embedding = self.encode_query(query)\n",
        "\n",
        "        query_params = {\n",
        "            'query_embeddings': [query_embedding.tolist()],\n",
        "            'n_results': top_k\n",
        "        }\n",
        "\n",
        "        if filters:\n",
        "            query_params['where'] = filters\n",
        "\n",
        "        results = self.collection.query(**query_params)\n",
        "\n",
        "        retrieved_docs = []\n",
        "        for i, (doc_id, document, metadata, distance) in enumerate(zip(\n",
        "            results['ids'][0],\n",
        "            results['documents'][0],\n",
        "            results['metadatas'][0],\n",
        "            results['distances'][0]\n",
        "        )):\n",
        "            retrieved_docs.append({\n",
        "                'rank': i + 1,\n",
        "                'doc_id': doc_id,\n",
        "                'text': document,\n",
        "                'similarity': 1 - distance,\n",
        "                'distance': distance,\n",
        "                'metadata': metadata\n",
        "            })\n",
        "\n",
        "        return retrieved_docs, time.time() - start_time\n",
        "\n",
        "    def format_context(self, documents: List[Dict]) -> str:\n",
        "        \"\"\"Format retrieved documents into context string.\"\"\"\n",
        "        context_parts = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for doc in documents:\n",
        "            doc_text = f\"\"\"\n",
        "Document {doc['rank']} [Disease: {doc['metadata'].get('disease_category', 'Unknown')}]:\n",
        "{doc['text']}\n",
        "---\n",
        "\"\"\"\n",
        "            doc_tokens = len(doc_text) // 4\n",
        "\n",
        "            if current_tokens + doc_tokens > self.config.MAX_CONTEXT_TOKENS:\n",
        "                remaining_chars = (self.config.MAX_CONTEXT_TOKENS - current_tokens) * 4\n",
        "                if remaining_chars > 100:\n",
        "                    doc_text = doc_text[:remaining_chars] + \"...\\n---\\n\"\n",
        "                    context_parts.append(doc_text)\n",
        "                break\n",
        "\n",
        "            context_parts.append(doc_text)\n",
        "            current_tokens += doc_tokens\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def create_prompt(self, query: str, context: str) -> str:\n",
        "        \"\"\"Create Mistral-formatted prompt.\"\"\"\n",
        "        system_instruction = \"\"\"You are a clinical AI assistant with expertise in medical diagnostics and patient care. Your role is to provide accurate, evidence-based answers using the provided clinical notes.\n",
        "\n",
        "Guidelines:\n",
        "- Base your answers strictly on the provided clinical context\n",
        "- Cite specific information from the documents when possible\n",
        "- Use clear, professional medical terminology\n",
        "- If the context doesn't contain sufficient information, clearly state what's missing\n",
        "- Never fabricate medical information or make unsupported claims\n",
        "- Consider differential diagnoses when appropriate\n",
        "- Acknowledge uncertainty when present in the data\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"<s>[INST] {system_instruction}\n",
        "\n",
        "Clinical Context from Patient Records:\n",
        "{context}\n",
        "\n",
        "Based on the clinical context above, answer the following question:\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Provide a clear, structured, evidence-based answer. [/INST]\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 5,\n",
        "                       filters: Optional[Dict] = None,\n",
        "                       show_progress: bool = True) -> Dict:\n",
        "        \"\"\"Complete RAG pipeline: retrieve, format, and generate.\"\"\"\n",
        "        pipeline_start = time.time()\n",
        "\n",
        "        if show_progress:\n",
        "            print(f\"\\nðŸ” Processing query: '{query}'\")\n",
        "\n",
        "        try:\n",
        "            # Retrieval\n",
        "            if show_progress:\n",
        "                print(\"   [1/3] Retrieving documents...\")\n",
        "\n",
        "            documents, retrieval_time = self.retrieve_documents(query, top_k, filters)\n",
        "\n",
        "            if show_progress:\n",
        "                print(f\"   âœ“ Retrieved {len(documents)} documents ({retrieval_time:.2f}s)\")\n",
        "\n",
        "            if not documents:\n",
        "                return {\n",
        "                    'query': query,\n",
        "                    'answer': \"No relevant documents found in the database.\",\n",
        "                    'sources': [],\n",
        "                    'metadata': {\n",
        "                        'retrieval_time': retrieval_time,\n",
        "                        'generation_time': 0,\n",
        "                        'total_time': time.time() - pipeline_start,\n",
        "                        'error': 'No documents retrieved'\n",
        "                    }\n",
        "                }\n",
        "\n",
        "            # Format context\n",
        "            if show_progress:\n",
        "                print(\"   [2/3] Formatting context...\")\n",
        "            context = self.format_context(documents)\n",
        "\n",
        "            # Generate\n",
        "            if show_progress:\n",
        "                print(\"   [3/3] Generating answer...\")\n",
        "\n",
        "            prompt = self.create_prompt(query, context)\n",
        "\n",
        "            generation_start = time.time()\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=self.config.MAX_INPUT_LENGTH\n",
        "            ).to(self.generation_model.device)\n",
        "\n",
        "            input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "            gen_config = {\n",
        "                'max_new_tokens': self.config.MAX_NEW_TOKENS,\n",
        "                'temperature': self.config.TEMPERATURE,\n",
        "                'top_p': self.config.TOP_P,\n",
        "                'top_k': self.config.TOP_K,\n",
        "                'repetition_penalty': self.config.REPETITION_PENALTY,\n",
        "                'do_sample': True,\n",
        "                'pad_token_id': self.tokenizer.pad_token_id,\n",
        "                'eos_token_id': self.tokenizer.eos_token_id,\n",
        "            }\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.generation_model.generate(**inputs, **gen_config)\n",
        "\n",
        "            generated_text = self.tokenizer.decode(\n",
        "                outputs[0][input_length:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            generation_time = time.time() - generation_start\n",
        "            output_length = len(outputs[0]) - input_length\n",
        "\n",
        "            if show_progress:\n",
        "                print(f\"   âœ“ Generated {output_length} tokens ({generation_time:.2f}s)\")\n",
        "\n",
        "            # Cleanup\n",
        "            del inputs, outputs\n",
        "            cleanup_memory()\n",
        "\n",
        "            # Statistics\n",
        "            self.stats['queries_processed'] += 1\n",
        "            self.stats['total_retrieval_time'] += retrieval_time\n",
        "            self.stats['total_generation_time'] += generation_time\n",
        "            self.stats['total_tokens_generated'] += output_length\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': generated_text.strip(),\n",
        "                'sources': documents,\n",
        "                'metadata': {\n",
        "                    'retrieval_time': retrieval_time,\n",
        "                    'generation_time': generation_time,\n",
        "                    'total_time': time.time() - pipeline_start,\n",
        "                    'input_tokens': input_length,\n",
        "                    'output_tokens': output_length,\n",
        "                    'documents_retrieved': len(documents),\n",
        "                    'success': True\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            if show_progress:\n",
        "                print(f\"   âŒ Error: {str(e)}\")\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': f\"ERROR: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'metadata': {\n",
        "                    'error': str(e),\n",
        "                    'success': False\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def print_result(self, result: Dict):\n",
        "        \"\"\"Pretty print a query result.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"QUERY RESULT\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nâ“ Query: {result['query']}\")\n",
        "        print(f\"\\nðŸ’¬ Answer:\\n{result['answer']}\")\n",
        "\n",
        "        if result['sources']:\n",
        "            print(f\"\\nðŸ“š Sources ({len(result['sources'])} documents):\")\n",
        "            for source in result['sources'][:3]:  # Show top 3\n",
        "                print(f\"\\n  â€¢ Document {source['rank']} (Similarity: {source['similarity']:.3f})\")\n",
        "                print(f\"    Category: {source['metadata'].get('disease_category', 'N/A')}\")\n",
        "                print(f\"    Preview: {source['text'][:150]}...\")\n",
        "\n",
        "        if 'metadata' in result and result['metadata'].get('success'):\n",
        "            meta = result['metadata']\n",
        "            print(f\"\\nâ±ï¸  Performance:\")\n",
        "            print(f\"    Retrieval: {meta['retrieval_time']:.2f}s\")\n",
        "            print(f\"    Generation: {meta['generation_time']:.2f}s\")\n",
        "            print(f\"    Total: {meta['total_time']:.2f}s\")\n",
        "            print(f\"    Tokens: {meta['output_tokens']} generated\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"âœ“ RAGPipeline class loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtPF_rPhD7E2",
        "outputId": "7d1940f7-6d49-4602-ca95-d767c75b6d93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ RAGPipeline class loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "CELL 11B: CLEAR CHROMADB SINGLETON (Run if you get errors)\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Clear ChromaDB singleton\n",
        "import chromadb\n",
        "from chromadb.api.client import SharedSystemClient\n",
        "\n",
        "# Method 1: Clear internal cache\n",
        "if hasattr(SharedSystemClient, '_identifer_to_system'):\n",
        "    SharedSystemClient._identifer_to_system.clear()\n",
        "    print(\"âœ“ Cleared _identifer_to_system cache\")\n",
        "\n",
        "# Method 2: Clear system cache\n",
        "if hasattr(SharedSystemClient, 'clear_system_cache'):\n",
        "    SharedSystemClient.clear_system_cache()\n",
        "    print(\"âœ“ Cleared system cache\")\n",
        "\n",
        "# Method 3: Delete and recreate (nuclear option)\n",
        "import sys\n",
        "if 'chromadb' in sys.modules:\n",
        "    del sys.modules['chromadb']\n",
        "    import chromadb\n",
        "    print(\"âœ“ Reimported chromadb module\")\n",
        "\n",
        "cleanup_memory()\n",
        "print(\"\\nâœ… ChromaDB singleton reset complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAOuyx4GLUl6",
        "outputId": "f50bdb7c-a07f-4cc2-ad24-6dc2af6d3bf8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Cleared system cache\n",
            "âœ“ Reimported chromadb module\n",
            "\n",
            "âœ… ChromaDB singleton reset complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 12: INITIALIZE RAG PIPELINE"
      ],
      "metadata": {
        "id": "TH_Cjm1WD-tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"INITIALIZING COMPLETE RAG PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize pipeline\n",
        "rag_pipeline = RAGPipeline(config)\n",
        "\n",
        "# Load models (this will take 3-5 minutes on first run)\n",
        "rag_pipeline.load_models(load_generation_model=True)\n",
        "\n",
        "print(\"\\nâœ… RAG Pipeline is ready!\")\n",
        "print(\"ðŸ’¡ Available as: rag_pipeline\")\n",
        "\n",
        "# Show GPU memory after loading\n",
        "if torch.cuda.is_available():\n",
        "    mem = get_gpu_memory()\n",
        "    print(f\"\\nðŸ“Š Final GPU Memory Usage:\")\n",
        "    print(f\"   Allocated: {mem['allocated']:.2f}GB\")\n",
        "    print(f\"   Total: {mem['total']:.2f}GB\")\n",
        "    print(f\"   Free: {mem['free']:.2f}GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411,
          "referenced_widgets": [
            "00876cfd527a4ada8749232b67af296f",
            "5a117a98f5e7486da617a0c9e4c63672",
            "4efb3b78317c4bfea5459b2a922d6a62",
            "31ef33c5aa0c41d980bde0a6e3adc1ac",
            "7fad0ff974d24b6e84397b991db5238d",
            "2773493081db4e7995ae9cf4928c60b6",
            "429ab2b09d2d41abae272286b818f7e4",
            "e0eaa9c24fa84aa0aec516f5d610bdeb",
            "3a8eba9a833c4c47b2eb3246c068d603",
            "db2be1303ca74f979509047d9fcae16d",
            "8e5802e777064950bda6ef45de0e590b",
            "1e22cb1f9e0849dab05bd6d60952add2",
            "b7ca8e508d9d4e64b69d61b08ae76808",
            "75ae0bd0f7a24ee8952a3d22abdce582",
            "9cb4ca3ca94041cda33451489c40f575",
            "0ebf4fb42e1c415291c804b9b1763aba",
            "918ba2e9c1134e67bf57619d0696cf3b",
            "703103da3dcc4eed950ef8a3927ceda4",
            "eb97fbdeb1e9442b851c4e21d28d843f",
            "186fba0696184dc29ae0b069cb44a5ed",
            "f7a132c733524184a8aeea8567471b77",
            "95ee2c54d2bc4448aaab7e14f9db74a4"
          ]
        },
        "id": "xR9qLFUMEG4t",
        "outputId": "ac933658-2a97-4b69-9e93-6c326c7561f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "INITIALIZING COMPLETE RAG PIPELINE\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING RAG PIPELINE MODELS\n",
            "================================================================================\n",
            "\n",
            "[1/3] ðŸ“¥ Loading embedding model...\n",
            "      âœ“ Embedding model loaded\n",
            "      GPU Memory: 0.13GB / 15.83GB\n",
            "\n",
            "[2/3] ðŸ“‚ Loading ChromaDB collection...\n",
            "      âœ“ Collection loaded (934 documents)\n",
            "\n",
            "[3/3] ðŸ¤– Loading Mistral-7B (4-bit quantization)...\n",
            "      â± This may take 2-5 minutes...\n",
            "      Loading tokenizer...\n",
            "      âœ“ Tokenizer loaded\n",
            "      Loading Mistral-7B model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00876cfd527a4ada8749232b67af296f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e22cb1f9e0849dab05bd6d60952add2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 13: TEST RAG PIPELINE WITH SAMPLE QUERIES"
      ],
      "metadata": {
        "id": "N-9TsdnSEI_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TESTING RAG PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define test queries\n",
        "test_queries = [\n",
        "    \"What are the common symptoms of pneumonia?\",\n",
        "    \"How is heart failure typically treated?\",\n",
        "    \"What are the complications of diabetes mellitus?\",\n",
        "    \"Describe the diagnostic criteria for sepsis\",\n",
        "    \"What medications are used for hypertension?\"\n",
        "]\n",
        "\n",
        "print(f\"\\nðŸ§ª Running {len(test_queries)} test queries...\\n\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TEST QUERY {i}/{len(test_queries)}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Generate answer\n",
        "    result = rag_pipeline.generate_answer(query, top_k=5, show_progress=True)\n",
        "    results.append(result)\n",
        "\n",
        "    # Print result\n",
        "    rag_pipeline.print_result(result)\n",
        "\n",
        "    # Short pause between queries\n",
        "    time.sleep(1)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal queries processed: {rag_pipeline.stats['queries_processed']}\")\n",
        "print(f\"Average retrieval time: {rag_pipeline.stats['total_retrieval_time'] / len(results):.2f}s\")\n",
        "print(f\"Average generation time: {rag_pipeline.stats['total_generation_time'] / len(results):.2f}s\")\n",
        "print(f\"Total tokens generated: {rag_pipeline.stats['total_tokens_generated']}\")\n",
        "print(f\"Average tokens per query: {rag_pipeline.stats['total_tokens_generated'] / len(results):.0f}\")\n"
      ],
      "metadata": {
        "id": "yjcRBrQ_EKVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 14: INTERACTIVE QUERY INTERFACE (COLAB)"
      ],
      "metadata": {
        "id": "7MOOIxbvEMvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def create_query_interface():\n",
        "    \"\"\"Create an interactive query interface for Colab.\"\"\"\n",
        "\n",
        "    # Widgets\n",
        "    query_input = widgets.Textarea(\n",
        "        value='',\n",
        "        placeholder='Enter your clinical question here...',\n",
        "        description='Query:',\n",
        "        layout=widgets.Layout(width='100%', height='80px')\n",
        "    )\n",
        "\n",
        "    top_k_slider = widgets.IntSlider(\n",
        "        value=5,\n",
        "        min=1,\n",
        "        max=10,\n",
        "        step=1,\n",
        "        description='Documents:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    category_dropdown = widgets.Dropdown(\n",
        "        options=['All'] + sorted(processed_df['disease_category'].unique().tolist()),\n",
        "        value='All',\n",
        "        description='Category:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    submit_button = widgets.Button(\n",
        "        description='Generate Answer',\n",
        "        button_style='primary',\n",
        "        icon='search'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_submit(button):\n",
        "        \"\"\"Handle query submission.\"\"\"\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "\n",
        "            query = query_input.value.strip()\n",
        "            if not query:\n",
        "                print(\"âš ï¸ Please enter a query\")\n",
        "                return\n",
        "\n",
        "            # Prepare filters\n",
        "            filters = None\n",
        "            if category_dropdown.value != 'All':\n",
        "                filters = {'disease_category': category_dropdown.value}\n",
        "\n",
        "            # Generate answer\n",
        "            result = rag_pipeline.generate_answer(\n",
        "                query,\n",
        "                top_k=top_k_slider.value,\n",
        "                filters=filters,\n",
        "                show_progress=True\n",
        "            )\n",
        "\n",
        "            # Display result\n",
        "            rag_pipeline.print_result(result)\n",
        "\n",
        "    submit_button.on_click(on_submit)\n",
        "\n",
        "    # Layout\n",
        "    interface = widgets.VBox([\n",
        "        widgets.HTML(\"<h2>ðŸ¥ Clinical RAG System - Interactive Query Interface</h2>\"),\n",
        "        query_input,\n",
        "        widgets.HBox([top_k_slider, category_dropdown]),\n",
        "        submit_button,\n",
        "        output_area\n",
        "    ])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create and display interface\n",
        "print(\"ðŸŽ¯ Creating interactive query interface...\")\n",
        "query_interface = create_query_interface()\n",
        "display(query_interface)"
      ],
      "metadata": {
        "id": "uJBL4wijEOSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 15: STREAMLIT APP CODE GENERATOR"
      ],
      "metadata": {
        "id": "LmuZmrU5EQT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_streamlit_app():\n",
        "    \"\"\"Generate Streamlit app code.\"\"\"\n",
        "\n",
        "    streamlit_code = '''\n",
        "import streamlit as st\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Add your imports and RAGPipeline class here\n",
        "# Copy the Config and RAGPipeline classes from the Colab notebook\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Clinical RAG System\",\n",
        "    page_icon=\"ðŸ¥\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        color: #1f77b4;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .query-box {\n",
        "        background-color: #f0f2f6;\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "    .source-card {\n",
        "        background-color: #ffffff;\n",
        "        border: 1px solid #e0e0e0;\n",
        "        border-radius: 0.5rem;\n",
        "        padding: 1rem;\n",
        "        margin: 0.5rem 0;\n",
        "    }\n",
        "    .metric-card {\n",
        "        background-color: #f8f9fa;\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        text-align: center;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if 'rag_pipeline' not in st.session_state:\n",
        "    st.session_state.rag_pipeline = None\n",
        "    st.session_state.query_history = []\n",
        "    st.session_state.models_loaded = False\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.title(\"âš™ï¸ Settings\")\n",
        "\n",
        "    # Model loading\n",
        "    if not st.session_state.models_loaded:\n",
        "        if st.button(\"ðŸš€ Load Models\", type=\"primary\"):\n",
        "            with st.spinner(\"Loading models... This may take 3-5 minutes...\"):\n",
        "                try:\n",
        "                    config = Config()  # Use your Config class\n",
        "                    st.session_state.rag_pipeline = RAGPipeline(config)\n",
        "                    st.session_state.rag_pipeline.load_models(load_generation_model=True)\n",
        "                    st.session_state.models_loaded = True\n",
        "                    st.success(\"âœ… Models loaded successfully!\")\n",
        "                    st.rerun()\n",
        "                except Exception as e:\n",
        "                    st.error(f\"âŒ Error loading models: {str(e)}\")\n",
        "    else:\n",
        "        st.success(\"âœ… Models loaded\")\n",
        "\n",
        "        # Query parameters\n",
        "        st.subheader(\"Query Parameters\")\n",
        "        top_k = st.slider(\"Number of documents\", 1, 10, 5)\n",
        "\n",
        "        # Optional: Category filter\n",
        "        categories = [\"All\"] + [\"Cardiovascular\", \"Respiratory\", \"Endocrine\"]  # Add your categories\n",
        "        category_filter = st.selectbox(\"Disease Category\", categories)\n",
        "\n",
        "        # Clear history\n",
        "        if st.button(\"ðŸ—‘ï¸ Clear History\"):\n",
        "            st.session_state.query_history = []\n",
        "            st.rerun()\n",
        "\n",
        "# Main content\n",
        "st.markdown('<h1 class=\"main-header\">ðŸ¥ Clinical RAG System</h1>', unsafe_allow_html=True)\n",
        "\n",
        "# Check if models are loaded\n",
        "if not st.session_state.models_loaded:\n",
        "    st.info(\"ðŸ‘ˆ Please load the models from the sidebar to begin.\")\n",
        "    st.stop()\n",
        "\n",
        "# Query input\n",
        "st.subheader(\"ðŸ’¬ Ask a Clinical Question\")\n",
        "\n",
        "col1, col2 = st.columns([4, 1])\n",
        "\n",
        "with col1:\n",
        "    query = st.text_area(\n",
        "        \"Enter your question:\",\n",
        "        height=100,\n",
        "        placeholder=\"e.g., What are the symptoms of pneumonia?\"\n",
        "    )\n",
        "\n",
        "with col2:\n",
        "    st.write(\"\")  # Spacing\n",
        "    st.write(\"\")  # Spacing\n",
        "    submit = st.button(\"ðŸ” Generate Answer\", type=\"primary\", use_container_width=True)\n",
        "\n",
        "# Process query\n",
        "if submit and query:\n",
        "    with st.spinner(\"ðŸ”„ Processing your query...\"):\n",
        "        try:\n",
        "            # Prepare filters\n",
        "            filters = None\n",
        "            if category_filter != \"All\":\n",
        "                filters = {\"disease_category\": category_filter}\n",
        "\n",
        "            # Generate answer\n",
        "            result = st.session_state.rag_pipeline.generate_answer(\n",
        "                query,\n",
        "                top_k=top_k,\n",
        "                filters=filters,\n",
        "                show_progress=False\n",
        "            )\n",
        "\n",
        "            # Add to history\n",
        "            st.session_state.query_history.insert(0, result)\n",
        "\n",
        "            # Display results\n",
        "            st.markdown(\"---\")\n",
        "\n",
        "            # Answer\n",
        "            st.subheader(\"ðŸ’¡ Answer\")\n",
        "            st.markdown(f'<div class=\"query-box\">{result[\"answer\"]}</div>', unsafe_allow_html=True)\n",
        "\n",
        "            # Metrics\n",
        "            if result[\"metadata\"].get(\"success\"):\n",
        "                col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "                with col1:\n",
        "                    st.metric(\"â±ï¸ Total Time\", f\"{result['metadata']['total_time']:.2f}s\")\n",
        "                with col2:\n",
        "                    st.metric(\"ðŸ“¥ Retrieval\", f\"{result['metadata']['retrieval_time']:.2f}s\")\n",
        "                with col3:\n",
        "                    st.metric(\"ðŸ¤– Generation\", f\"{result['metadata']['generation_time']:.2f}s\")\n",
        "                with col4:\n",
        "                    st.metric(\"ðŸ“ Tokens\", result['metadata']['output_tokens'])\n",
        "\n",
        "            # Sources\n",
        "            st.subheader(\"ðŸ“š Sources\")\n",
        "            for i, source in enumerate(result['sources'][:5], 1):\n",
        "                with st.expander(f\"Document {i} - {source['metadata'].get('disease_category', 'N/A')} (Similarity: {source['similarity']:.3f})\"):\n",
        "                    st.markdown(f\"**Category:** {source['metadata'].get('disease_category', 'N/A')}\")\n",
        "                    st.markdown(f\"**Subtype:** {source['metadata'].get('disease_subtype', 'N/A')}\")\n",
        "                    st.markdown(\"**Content:**\")\n",
        "                    st.text(source['text'][:500] + \"...\" if len(source['text']) > 500 else source['text'])\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"âŒ Error: {str(e)}\")\n",
        "\n",
        "# Query history\n",
        "if st.session_state.query_history:\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"ðŸ“œ Query History\")\n",
        "\n",
        "    for i, past_result in enumerate(st.session_state.query_history[:5], 1):\n",
        "        with st.expander(f\"{i}. {past_result['query'][:80]}...\"):\n",
        "            st.markdown(f\"**Answer:** {past_result['answer'][:300]}...\")\n",
        "            if past_result['metadata'].get('success'):\n",
        "                st.caption(f\"â±ï¸ {past_result['metadata']['total_time']:.2f}s | \"\n",
        "                          f\"ðŸ“š {len(past_result['sources'])} sources\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"ðŸ¥ Clinical RAG System | Powered by MIMIC-IV-EXT, E5 Embeddings, and Mistral-7B\")\n",
        "'''\n",
        "\n",
        "    # Save to Google Drive\n",
        "    output_path = config.DRIVE_ROOT / \"streamlit_app.py\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(streamlit_code)\n",
        "\n",
        "    print(f\"âœ“ Streamlit app saved to: {output_path}\")\n",
        "    print(\"\\nðŸ“ To run the Streamlit app:\")\n",
        "    print(\"1. Install: !pip install streamlit\")\n",
        "    print(f\"2. Run: !streamlit run {output_path}\")\n",
        "    print(\"3. Or use ngrok/localtunnel to expose the app\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# Generate app\n",
        "print(\"=\"*80)\n",
        "print(\"GENERATING STREAMLIT APP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "app_path = generate_streamlit_app()\n",
        "\n",
        "print(\"\\nðŸ’¡ Next steps:\")\n",
        "print(\"1. Copy the Config and RAGPipeline classes into the Streamlit app\")\n",
        "print(\"2. Update the Google Drive paths in the Config class\")\n",
        "print(\"3. Run the app using: streamlit run streamlit_app.py\")"
      ],
      "metadata": {
        "id": "CPIau4X9ER7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 16: SAVE FINAL STATE"
      ],
      "metadata": {
        "id": "w52LhainEVE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pipeline_state():\n",
        "    \"\"\"Save pipeline state to Google Drive.\"\"\"\n",
        "\n",
        "    state = {\n",
        "        'config': {\n",
        "            'embedding_model': config.EMBEDDING_MODEL,\n",
        "            'generation_model': config.GENERATION_MODEL,\n",
        "            'paths': {\n",
        "                'processed_data': str(config.PROCESSED_DATA_DIR),\n",
        "                'chroma_db': str(config.CHROMA_DB_PATH),\n",
        "                'models_cache': str(config.MODELS_CACHE)\n",
        "            }\n",
        "        },\n",
        "        'stats': {\n",
        "            'processed_chunks': len(processed_df) if 'processed_df' in globals() else 0,\n",
        "            'embeddings_shape': embeddings.shape if 'embeddings' in globals() else None,\n",
        "            'chromadb_count': chroma_collection.count() if 'chroma_collection' in globals() else 0,\n",
        "            'queries_processed': rag_pipeline.stats['queries_processed'] if 'rag_pipeline' in globals() else 0\n",
        "        },\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    state_path = config.DRIVE_ROOT / \"pipeline_state.json\"\n",
        "    with open(state_path, 'w') as f:\n",
        "        json.dump(state, f, indent=2)\n",
        "\n",
        "    print(f\"âœ“ Pipeline state saved to: {state_path}\")\n",
        "    return state\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SAVING PIPELINE STATE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_state = save_pipeline_state()\n",
        "\n",
        "print(\"\\nðŸ“Š Final State Summary:\")\n",
        "print(json.dumps(final_state, indent=2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ALL SETUP COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸ’¡ Everything is cached in Google Drive:\")\n",
        "print(f\"   ðŸ“ Root: {config.DRIVE_ROOT}\")\n",
        "print(f\"   ðŸ“„ Processed data: {config.PROCESSED_DF_PATH}\")\n",
        "print(f\"   ðŸ”¢ Embeddings: {config.EMBEDDINGS_PATH}\")\n",
        "print(f\"   ðŸ—„ï¸ ChromaDB: {config.CHROMA_DB_PATH}\")\n",
        "print(f\"   ðŸ¤– Models cache: {config.MODELS_CACHE}\")\n",
        "print(\"\\nðŸš€ Next time you run this notebook, it will load from cache!\")\n",
        "print(\"âš¡ Subsequent runs will be 10-20x faster!\")\n"
      ],
      "metadata": {
        "id": "D4xcVTLlEXAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Streamlit app\n",
        "streamlit_code = generate_streamlit_app()\n",
        "streamlit_path = config.DRIVE_ROOT / \"streamlit_app.py\"\n",
        "\n",
        "with open(streamlit_path, 'w') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(f\"\\nâœ“ Streamlit app generated: {streamlit_path}\")\n",
        "print(f\"  Size: {streamlit_path.stat().st_size / 1024:.2f} KB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“± HOW TO RUN STREAMLIT IN COLAB\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. Install streamlit-localtunnel:\")\n",
        "print(\"   !pip install streamlit-localtunnel\")\n",
        "print(\"\\n2. Run in a new cell:\")\n",
        "print(\"   !streamlit run /content/drive/MyDrive/Clinical_RAG_System/streamlit_app.py &\")\n",
        "print(\"   !npx localtunnel --port 8501\")\n",
        "print(\"\\n3. Copy the generated URL and access your app!\")\n",
        "\n",
        "print(\"\\nâœ… All cells complete! Your RAG system is ready.\")"
      ],
      "metadata": {
        "id": "Gqp28c51EheE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit-localtunnel\n",
        "!streamlit run /content/drive/MyDrive/Clinical_RAG_System/streamlit_app.py &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "HwU2v00uPNp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}